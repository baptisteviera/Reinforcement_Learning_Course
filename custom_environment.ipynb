{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('Reinforcement_Learning_YT_course': conda)"
  },
  "interpreter": {
   "hash": "794c057f17fa386372f6b2ba86373d42751b29285ae33eea3ce01c3edd0521eb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "\n",
    "# 1. Import Dependencies¶\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "source": [
    "\n",
    "# 2. Types of Spaces\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "Discrete(3).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[100,  70, 196],\n",
       "       [206, 112, 204],\n",
       "       [ 57,  12, 117]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "Box(0,1,shape=(3,3)).sample()\n",
    "Box(0,255,shape=(3,3), dtype=int).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, array([46.174618], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "Tuple((Discrete(2), Box(0,100, shape=(1,)))).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('height', 1), ('speed', array([15.95309], dtype=float32))])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "Dict({'height':Discrete(2), \"speed\":Box(0,100, shape=(1,))}).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1], dtype=int8)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "MultiBinary(4).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "MultiDiscrete([5,2,2]).sample()"
   ]
  },
  {
   "source": [
    "with MultiDiscrete exemple :\n",
    "- first value : between 0 and 4\n",
    "- second value : between 0 and 1\n",
    "- third value : same as previous"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "# 3. Building an Environment¶\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Our goal is to build an agent ot give us the best shower possible\n",
    "- Randomly temperature\n",
    "- 37 and 39 degrees (the magic ratio)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Temperature array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # = Box(low = 0, high = 100, shape(1,)  )\n",
    "        # Set start temp\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Set shower length\n",
    "        self.shower_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "\n",
    "        # Apply action\n",
    "        # action - 1\n",
    "        # 0 -1 = -1 temperature -> decrease temperature (action = 0)\n",
    "        # 1 -1 = 0 -> no change temperature (action = 1)\n",
    "        # 2 -1 = 1 temperature -> increase temperature (action = 2)\n",
    "        \n",
    "        # the new temperature\n",
    "        self.state += action -1 \n",
    "        # Reduce shower length by 1 second\n",
    "        self.shower_length -= 1 \n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.state >=37 and self.state <=39: # it's the magic ratio. Cf markdown above\n",
    "            reward =1 \n",
    "        else: \n",
    "            reward = -1 \n",
    "        \n",
    "        # Check if shower is done\n",
    "        if self.shower_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Apply temperature noise\n",
    "        #self.state += random.randint(-1,1)\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = np.array([38 + random.randint(-3,3)]).astype(float)\n",
    "        # Reset shower time\n",
    "        self.shower_length = 60 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = ShowerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([82.28524], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "source": [
    "#  4. Test Environment\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode:1 Score:10\nEpisode:2 Score:-52\nEpisode:3 Score:-46\nEpisode:4 Score:-44\nEpisode:5 Score:-30\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n"
   ]
  },
  {
   "source": [
    "\n",
    "# 5. Train Model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training_Custom_Env', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\nWrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logging to Training_Custom_Env/Logs/PPO_3\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | 55.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 1680     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 56.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1245        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010633253 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 7.4e-05     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.9        |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    value_loss           | 93.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 56.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1234        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015956726 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 9.21e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.4        |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 53.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1225       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04374273 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.569     |\n",
      "|    explained_variance   | 1.67e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37.5       |\n",
      "|    n_updates            | 2970       |\n",
      "|    policy_gradient_loss | 0.0114     |\n",
      "|    value_loss           | 106        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 53.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1216       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02076978 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.524     |\n",
      "|    explained_variance   | 1.42e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 60.4       |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | 0.00507    |\n",
      "|    value_loss           | 108        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 50.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1211        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017882288 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.000184    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.9        |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 44.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1198        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034697626 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | 0.000162    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.5        |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | 0.008       |\n",
      "|    value_loss           | 99.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 44.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1185        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014713071 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.00043     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.2        |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | 0.000681    |\n",
      "|    value_loss           | 93.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 45.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1183         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049716234 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 0.000842     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.7         |\n",
      "|    n_updates            | 3020         |\n",
      "|    policy_gradient_loss | 0.00856      |\n",
      "|    value_loss           | 84.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 52         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1185       |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04972419 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.525     |\n",
      "|    explained_variance   | 0.00069    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 44.8       |\n",
      "|    n_updates            | 3030       |\n",
      "|    policy_gradient_loss | 0.00859    |\n",
      "|    value_loss           | 90         |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fdf0d7b6fd0>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "model.learn(total_timesteps=2000)"
   ]
  },
  {
   "source": [
    "\n",
    "# 6. Save Model¶\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_path = os.path.join('Training_Custom_Env','Saved Model','Shoxer_Model_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(shower_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\nWrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(shower_path,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(59.2, 0.9797958971132712)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "evaluate_policy(model, env,n_eval_episodes=10,render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}