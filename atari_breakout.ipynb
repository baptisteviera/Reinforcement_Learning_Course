{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gym # OpenAI gym\n",
    "from stable_baselines3 import A2C # on fait le choix de prendre cet algorithme parmi (PPO,ACER,DQN,....)\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "# in the course we didn't vectorized our environment, we only trained on one environment at a time\n",
    "# with breakout we are going to train on four environments at the same time in order to speed up our training\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # easier to test our model\n",
    "from stable_baselines3.common.env_util import make_atari_env\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Test Environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying tutankham.bin from ./ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/tutankham.bin\r\n",
      "copying phoenix.bin from ./ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/phoenix.bin\r\n",
      "copying robotank.bin from ./ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/robotank.bin\r\n",
      "copying qbert.bin from ./ROMS/Q-bert (1987) (Atari) (CX26150).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/qbert.bin\r\n",
      "copying adventure.bin from ./ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/adventure.bin\r\n",
      "copying journey_escape.bin from ./ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/journey_escape.bin\r\n",
      "copying skiing.bin from ./ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/skiing.bin\r\n",
      "copying kangaroo.bin from ./ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/kangaroo.bin\r\n",
      "copying amidar.bin from ./ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/amidar.bin\r\n",
      "copying keystone_kapers.bin from ./ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/keystone_kapers.bin\r\n",
      "copying lost_luggage.bin from ./ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/lost_luggage.bin\r\n",
      "copying video_pinball.bin from ./ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/video_pinball.bin\r\n",
      "copying kaboom.bin from ./ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/kaboom.bin\r\n",
      "copying galaxian.bin from ./ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/galaxian.bin\r\n",
      "copying venture.bin from ./ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/venture.bin\r\n",
      "copying defender.bin from ./ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/defender.bin\r\n",
      "copying bank_heist.bin from ./ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/bank_heist.bin\r\n",
      "copying solaris.bin from ./ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/solaris.bin\r\n",
      "copying surround.bin from ./ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/surround.bin\r\n",
      "copying krull.bin from ./ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/krull.bin\r\n",
      "copying yars_revenge.bin from ./ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/yars_revenge.bin\r\n",
      "copying riverraid.bin from ./ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/riverraid.bin\r\n",
      "copying ms_pacman.bin from ./ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/ms_pacman.bin\r\n",
      "copying alien.bin from ./ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/alien.bin\r\n",
      "copying donkey_kong.bin from ./ROMS/Donkey Kong (1987) (Atari) (CX26143).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/donkey_kong.bin\r\n",
      "copying road_runner.bin from patched version of ./ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/road_runner.bin\r\n",
      "copying bowling.bin from ./ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/bowling.bin\r\n",
      "copying frogger.bin from ./ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/frogger.bin\r\n",
      "copying atlantis.bin from ./ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/atlantis.bin\r\n",
      "copying double_dunk.bin from ./ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/double_dunk.bin\r\n",
      "copying trondead.bin from ./ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/trondead.bin\r\n",
      "copying time_pilot.bin from ./ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/time_pilot.bin\r\n",
      "copying koolaid.bin from ./ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/koolaid.bin\r\n",
      "copying asterix.bin from ./ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/asterix.bin\r\n",
      "copying gravitar.bin from ./ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/gravitar.bin\r\n",
      "copying freeway.bin from ./ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/freeway.bin\r\n",
      "copying seaquest.bin from ./ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/seaquest.bin\r\n",
      "copying pitfall.bin from ./ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/pitfall.bin\r\n",
      "copying breakout.bin from ./ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/breakout.bin\r\n",
      "copying berzerk.bin from ./ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/berzerk.bin\r\n",
      "copying ice_hockey.bin from ./ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/ice_hockey.bin\r\n",
      "copying battle_zone.bin from ./ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/battle_zone.bin\r\n",
      "copying assault.bin from ./ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/assault.bin\r\n",
      "copying up_n_down.bin from ./ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/up_n_down.bin\r\n",
      "copying carnival.bin from ./ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/carnival.bin\r\n",
      "copying jamesbond.bin from ./ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/jamesbond.bin\r\n",
      "copying asteroids.bin from ./ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/asteroids.bin\r\n",
      "copying demon_attack.bin from ./ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/demon_attack.bin\r\n",
      "copying fishing_derby.bin from ./ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/fishing_derby.bin\r\n",
      "copying zaxxon.bin from ./ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/zaxxon.bin\r\n",
      "copying montezuma_revenge.bin from ./ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/montezuma_revenge.bin\r\n",
      "copying pacman.bin from ./ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/pacman.bin\r\n",
      "copying sir_lancelot.bin from ./ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/sir_lancelot.bin\r\n",
      "copying mr_do.bin from ./ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/mr_do.bin\r\n",
      "copying elevator_action.bin from ./ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/elevator_action.bin\r\n",
      "copying space_invaders.bin from ./ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/space_invaders.bin\r\n",
      "copying enduro.bin from ./ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/enduro.bin\r\n",
      "copying beam_rider.bin from ./ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/beam_rider.bin\r\n",
      "copying hero.bin from ./ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/hero.bin\r\n",
      "copying air_raid.bin from ./ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/air_raid.bin\r\n",
      "copying private_eye.bin from ./ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/private_eye.bin\r\n",
      "copying pooyan.bin from ./ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/pooyan.bin\r\n",
      "copying centipede.bin from ./ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/centipede.bin\r\n",
      "copying chopper_command.bin from ./ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/chopper_command.bin\r\n",
      "copying star_gunner.bin from ./ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/star_gunner.bin\r\n",
      "copying gopher.bin from ./ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/gopher.bin\r\n",
      "copying pong.bin from ./ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/pong.bin\r\n",
      "copying name_this_game.bin from ./ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/name_this_game.bin\r\n",
      "copying tennis.bin from ./ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/tennis.bin\r\n",
      "copying kung_fu_master.bin from ./ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/kung_fu_master.bin\r\n",
      "copying boxing.bin from ./ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/boxing.bin\r\n",
      "copying crazy_climber.bin from ./ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/crazy_climber.bin\r\n",
      "copying king_kong.bin from ./ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/king_kong.bin\r\n",
      "copying laser_gates.bin from ./ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/laser_gates.bin\r\n",
      "copying frostbite.bin from ./ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/frostbite.bin\r\n",
      "copying wizard_of_wor.bin from ./ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /home/uruk380/anaconda3/envs/Reinforcement_Learning_YT_course/lib/python3.8/site-packages/atari_py/atari_roms/wizard_of_wor.bin\r\n"
     ]
    }
   ],
   "source": [
    "!python -m atari_py.import_roms ./ROMS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "environment_name = 'Breakout-v0'\n",
    "env = gym.make(environment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]],\n\n       [[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        ...,\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]]], dtype=uint8)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(4)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Box(0, 255, (210, 160, 3), uint8)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space\n",
    "# Box(0,255,(210,160,3),unit8)\n",
    "# lowbound = 0 , upperbound = 255\n",
    "# 210 -> height\n",
    "# 160 -> width\n",
    "# 3 -> an image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 0.0\n",
      "Episode: 2 Score: 0.0\n",
      "Episode: 3 Score: 2.0\n",
      "Episode: 4 Score: 0.0\n",
      "Episode: 5 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5  # we test the cartpole environnement five times\n",
    "# an episode must be seen as one full game within the environment\n",
    "# same environments have a fixed episode length\n",
    "    # CartPole which is 200 frames (length of an episode)\n",
    "    # others are continuous\n",
    "        # Breakout play until you run out of lives\n",
    "for episode in range(1,episodes+1):\n",
    "    obs = env.reset() # we have an initial set of observation for the environment\n",
    "    done = False # is our episode is done ?\n",
    "    score = 0 # compter\n",
    "\n",
    "    while not done:\n",
    "        # allow to view the graphical representation of the environment\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        # return the agent's observation (the next observation)\n",
    "        # obs = array of the new observation space\n",
    "        # reward = 1 or 0\n",
    "        # done = whether or not the episode is done -> if True we go out of the while loop\n",
    "        # info is a dict\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # we accumulate and save the total amount of reward\n",
    "        score += reward\n",
    "    print('Episode: {} Score: {}'.format(episode,score))\n",
    "env.close()\n",
    "\n",
    "# the score is very low ^^"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Vectorise Environment and Train Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=4,seed=0)\n",
    "env = VecFrameStack(env,n_stack=4)\n",
    "\n",
    "# Helper Functions\n",
    "    # make_atari_env is a helper from stable baselines that helps create wrapped Atari environments\n",
    "    # VecframeStack allows us to stack the environments together"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        ...,\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]]],\n\n\n       [[[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        ...,\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]]],\n\n\n       [[[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        ...,\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]]],\n\n\n       [[[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        ...,\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]],\n\n        [[0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         ...,\n         [0, 0, 0, 0],\n         [0, 0, 0, 0],\n         [0, 0, 0, 0]]]], dtype=uint8)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # not to forget !"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(0.1)\n",
    "env.render()\n",
    "# becareful render doesn't work on ubuntu :(\n",
    "# i have a black window"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorizing\n",
    "    Vectorizing the environnement particulary with multiple environments,\n",
    "    allows you to train the agent faster by training in parallel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join(\"Training_Atari_Breakout\",\"Logs\")\n",
    "model = A2C('CnnPolicy',env, verbose=1,tensorboard_log=log_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training_Atari_Breakout/Logs/A2C_2\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 553      |\n",
      "|    ep_rew_mean        | 7.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 293      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.336   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -0.0388  |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 538      |\n",
      "|    ep_rew_mean        | 7.06     |\n",
      "| time/                 |          |\n",
      "|    fps                | 296      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.286   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 527      |\n",
      "|    ep_rew_mean        | 6.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 301      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -0.00734 |\n",
      "|    value_loss         | 0.0317   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 524      |\n",
      "|    ep_rew_mean        | 6.85     |\n",
      "| time/                 |          |\n",
      "|    fps                | 304      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.632   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.0145   |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 510      |\n",
      "|    ep_rew_mean        | 6.51     |\n",
      "| time/                 |          |\n",
      "|    fps                | 298      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.492   |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.0651   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 502      |\n",
      "|    ep_rew_mean        | 6.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 297      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.755   |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 0.085    |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 499      |\n",
      "|    ep_rew_mean        | 6.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.406   |\n",
      "|    explained_variance | 0.654    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 0.131    |\n",
      "|    value_loss         | 0.259    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 492       |\n",
      "|    ep_rew_mean        | 6.21      |\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.487    |\n",
      "|    explained_variance | 0.957     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -2.21e-05 |\n",
      "|    value_loss         | 0.0136    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 489      |\n",
      "|    ep_rew_mean        | 6.12     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.674   |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | 0.0801   |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 493      |\n",
      "|    ep_rew_mean        | 6.14     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.774   |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 0.21     |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 497      |\n",
      "|    ep_rew_mean        | 6.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.805   |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | -0.195   |\n",
      "|    value_loss         | 0.149    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | 6.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 81       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.653   |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 0.0757   |\n",
      "|    value_loss         | 0.0738   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 500      |\n",
      "|    ep_rew_mean        | 6.23     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.511   |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.00154  |\n",
      "|    value_loss         | 0.0646   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 508      |\n",
      "|    ep_rew_mean        | 6.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 95       |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.601   |\n",
      "|    explained_variance | 0.711    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -0.127   |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 518      |\n",
      "|    ep_rew_mean        | 6.51     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.474   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | -0.025   |\n",
      "|    value_loss         | 0.0261   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 531      |\n",
      "|    ep_rew_mean        | 6.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 0.366    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 0.0574   |\n",
      "|    value_loss         | 0.454    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 535      |\n",
      "|    ep_rew_mean        | 7.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 282      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.591   |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -0.247   |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 537      |\n",
      "|    ep_rew_mean        | 7.12     |\n",
      "| time/                 |          |\n",
      "|    fps                | 273      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.638   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 547      |\n",
      "|    ep_rew_mean        | 7.26     |\n",
      "| time/                 |          |\n",
      "|    fps                | 266      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.284   |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 0.0506   |\n",
      "|    value_loss         | 0.0814   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 529      |\n",
      "|    ep_rew_mean        | 6.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 155      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.661   |\n",
      "|    explained_variance | 0.742    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 0.0641   |\n",
      "|    value_loss         | 0.0736   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 511      |\n",
      "|    ep_rew_mean        | 6.58     |\n",
      "| time/                 |          |\n",
      "|    fps                | 250      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.447   |\n",
      "|    explained_variance | 0.713    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | -0.0396  |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 496      |\n",
      "|    ep_rew_mean        | 6.18     |\n",
      "| time/                 |          |\n",
      "|    fps                | 244      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 179      |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.259   |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 0.00791  |\n",
      "|    value_loss         | 0.0441   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 496      |\n",
      "|    ep_rew_mean        | 6.12     |\n",
      "| time/                 |          |\n",
      "|    fps                | 241      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 190      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 0.113    |\n",
      "|    value_loss         | 0.058    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 498      |\n",
      "|    ep_rew_mean        | 6.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 238      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 201      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.665   |\n",
      "|    explained_variance | 0.723    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.179    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 514       |\n",
      "|    ep_rew_mean        | 6.57      |\n",
      "| time/                 |           |\n",
      "|    fps                | 236       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 211       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.526    |\n",
      "|    explained_variance | 0.799     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -0.000983 |\n",
      "|    value_loss         | 0.0461    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 526      |\n",
      "|    ep_rew_mean        | 6.74     |\n",
      "| time/                 |          |\n",
      "|    fps                | 234      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 221      |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | 0.563    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 0.124    |\n",
      "|    value_loss         | 0.312    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 531      |\n",
      "|    ep_rew_mean        | 6.97     |\n",
      "| time/                 |          |\n",
      "|    fps                | 236      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 228      |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.279   |\n",
      "|    explained_variance | 0.754    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 0.0204   |\n",
      "|    value_loss         | 0.0886   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 549      |\n",
      "|    ep_rew_mean        | 7.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 238      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 234      |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.309   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -0.0176  |\n",
      "|    value_loss         | 0.105    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 531      |\n",
      "|    ep_rew_mean        | 7.06     |\n",
      "| time/                 |          |\n",
      "|    fps                | 240      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 241      |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.332   |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 0.0219   |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 527      |\n",
      "|    ep_rew_mean        | 6.96     |\n",
      "| time/                 |          |\n",
      "|    fps                | 241      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 248      |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.297   |\n",
      "|    explained_variance | 0.725    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -0.0345  |\n",
      "|    value_loss         | 0.189    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 516      |\n",
      "|    ep_rew_mean        | 6.73     |\n",
      "| time/                 |          |\n",
      "|    fps                | 243      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 254      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | 0.687    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 0.00858  |\n",
      "|    value_loss         | 0.026    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 513      |\n",
      "|    ep_rew_mean        | 6.6      |\n",
      "| time/                 |          |\n",
      "|    fps                | 245      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 261      |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.38    |\n",
      "|    explained_variance | 0.769    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | 0.0436   |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 513      |\n",
      "|    ep_rew_mean        | 6.46     |\n",
      "| time/                 |          |\n",
      "|    fps                | 246      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 267      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.407   |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -0.0444  |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 506      |\n",
      "|    ep_rew_mean        | 6.3      |\n",
      "| time/                 |          |\n",
      "|    fps                | 246      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 275      |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.209   |\n",
      "|    explained_variance | 0.96     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | -0.00679 |\n",
      "|    value_loss         | 0.0753   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 533      |\n",
      "|    ep_rew_mean        | 6.79     |\n",
      "| time/                 |          |\n",
      "|    fps                | 248      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 282      |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.46    |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 0.0526   |\n",
      "|    value_loss         | 0.0858   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 540      |\n",
      "|    ep_rew_mean        | 7.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 249      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 288      |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.242   |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | 0.0137   |\n",
      "|    value_loss         | 0.0873   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 555      |\n",
      "|    ep_rew_mean        | 7.2      |\n",
      "| time/                 |          |\n",
      "|    fps                | 250      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 295      |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.119   |\n",
      "|    explained_variance | 0.191    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 0.0217   |\n",
      "|    value_loss         | 0.279    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 559      |\n",
      "|    ep_rew_mean        | 7.38     |\n",
      "| time/                 |          |\n",
      "|    fps                | 250      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 303      |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.381   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | 0.0387   |\n",
      "|    value_loss         | 0.0363   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 569      |\n",
      "|    ep_rew_mean        | 7.7      |\n",
      "| time/                 |          |\n",
      "|    fps                | 251      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 309      |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.338   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | -0.024   |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 579      |\n",
      "|    ep_rew_mean        | 7.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 249      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 320      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.197   |\n",
      "|    explained_variance | 0.0906   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 0.024    |\n",
      "|    value_loss         | 0.122    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 575      |\n",
      "|    ep_rew_mean        | 7.94     |\n",
      "| time/                 |          |\n",
      "|    fps                | 247      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 330      |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.463   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.025    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 584      |\n",
      "|    ep_rew_mean        | 8.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 245      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 342      |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.283   |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | 0.103    |\n",
      "|    value_loss         | 0.204    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 583      |\n",
      "|    ep_rew_mean        | 8.06     |\n",
      "| time/                 |          |\n",
      "|    fps                | 243      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 352      |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.253   |\n",
      "|    explained_variance | 0.75     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | -0.0416  |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 581      |\n",
      "|    ep_rew_mean        | 8.12     |\n",
      "| time/                 |          |\n",
      "|    fps                | 243      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 361      |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | 0.603    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    value_loss         | 0.183    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 578      |\n",
      "|    ep_rew_mean        | 8.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 241      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 372      |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.43    |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -0.0346  |\n",
      "|    value_loss         | 0.0533   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 587      |\n",
      "|    ep_rew_mean        | 8.14     |\n",
      "| time/                 |          |\n",
      "|    fps                | 239      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 384      |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -0.0514  |\n",
      "|    value_loss         | 0.0431   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 585      |\n",
      "|    ep_rew_mean        | 7.95     |\n",
      "| time/                 |          |\n",
      "|    fps                | 238      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 394      |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.309   |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -0.0558  |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 563      |\n",
      "|    ep_rew_mean        | 7.46     |\n",
      "| time/                 |          |\n",
      "|    fps                | 236      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 406      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.599   |\n",
      "|    explained_variance | 0.232    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 0.139    |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 558      |\n",
      "|    ep_rew_mean        | 7.38     |\n",
      "| time/                 |          |\n",
      "|    fps                | 234      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 417      |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.453   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -0.0175  |\n",
      "|    value_loss         | 0.0234   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 565      |\n",
      "|    ep_rew_mean        | 7.47     |\n",
      "| time/                 |          |\n",
      "|    fps                | 232      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 429      |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.283   |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.0819   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.a2c.a2c.A2C at 0x7fa79a420c70>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)\n",
    "# much bigger the timesteps is much bigger is the result !"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Save and Reaload Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "a2c_path = os.path.join('Training_Atari_Breakout','Saved Models', 'A2C_Breakout_Model')\n",
    "model.save(a2c_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    " del model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = A2C.load(a2c_path,env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Evaluate and Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Becareful we can evaluate our policy with a vectorized environment\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "env = make_atari_env('Breakout-v0', n_envs=1,seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(6.7, 2.968164415931166)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model,env,n_eval_episodes=10,render=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "env.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}